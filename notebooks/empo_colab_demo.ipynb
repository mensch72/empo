{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMPO: Human Empowerment AI Agents - Google Colab Demo\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mensch72/empo/blob/main/notebooks/empo_colab_demo.ipynb)\n",
    "\n",
    "This notebook demonstrates how to set up and run the EMPO framework in Google Colab.\n",
    "\n",
    "**EMPO** (Empowerment-based Multi-Agent Reinforcement Learning) is a framework for studying the soft maximization of aggregate human power by AI agents.\n",
    "\n",
    "## Features demonstrated:\n",
    "- Setting up EMPO in Google Colab\n",
    "- Basic environment creation and state management\n",
    "- Computing the state-space DAG\n",
    "- Computing human policy priors via backward induction\n",
    "- Visualizing results\n",
    "- Neural network training with GPU and TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's clone the repository and install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the EMPO repository\n",
    "!git clone --depth 1 https://github.com/mensch72/empo.git\n",
    "%cd empo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install system dependencies (graphviz for DAG visualization)\n",
    "!apt-get update -qq && apt-get install -qq graphviz > /dev/null 2>&1\n",
    "print(\"✓ System dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python dependencies (using minimal Colab requirements)\n",
    "!pip install -q -r requirements-colab.txt\n",
    "print(\"✓ Python dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Python paths for EMPO imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src and vendor directories to Python path\n",
    "repo_root = os.getcwd()\n",
    "sys.path.insert(0, os.path.join(repo_root, 'src'))\n",
    "sys.path.insert(0, os.path.join(repo_root, 'vendor', 'multigrid'))\n",
    "\n",
    "print(f\"✓ PYTHONPATH configured\")\n",
    "print(f\"  Repository root: {repo_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Installation\n",
    "\n",
    "Let's verify that all components are properly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system info\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"System Information:\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  NumPy version: {np.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test EMPO imports\n",
    "try:\n",
    "    from empo import WorldModel, PossibleGoal, PossibleGoalGenerator, compute_human_policy_prior\n",
    "    print(\"✓ EMPO core imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ EMPO import error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MultiGrid imports\n",
    "try:\n",
    "    from gym_multigrid.multigrid import MultiGridEnv, Grid, Agent, Block, Rock, World\n",
    "    print(\"✓ MultiGrid imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ MultiGrid import error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test environment imports\n",
    "try:\n",
    "    from envs.one_or_three_chambers import SmallOneOrThreeChambersMapEnv\n",
    "    print(\"✓ Environment imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Environment import error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create and Explore an Environment\n",
    "\n",
    "Let's create a simple multi-agent gridworld environment and explore its state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small environment for demonstration\n",
    "from envs.one_or_three_chambers import SmallOneOrThreeChambersMapEnv\n",
    "\n",
    "env = SmallOneOrThreeChambersMapEnv()\n",
    "env.reset()\n",
    "\n",
    "print(\"Environment Information:\")\n",
    "print(f\"  Grid size: {env.width} x {env.height}\")\n",
    "print(f\"  Number of agents: {len(env.agents)}\")\n",
    "print(f\"  Max steps: {env.max_steps}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display agent information\n",
    "for i, agent in enumerate(env.agents):\n",
    "    agent_type = \"Human\" if agent.color == 'yellow' else \"AI\"\n",
    "    can_push = getattr(agent, 'can_push_rocks', False)\n",
    "    print(f\"Agent {i} ({agent_type}): pos={tuple(agent.pos)}, color={agent.color}, can_push_rocks={can_push}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the environment (initial state)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = env.render(mode='rgb_array', highlight=False)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.title('Initial Environment State')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. State Management\n",
    "\n",
    "EMPO extends MultiGrid with explicit state management. Let's explore the state representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current state\n",
    "state = env.get_state()\n",
    "\n",
    "print(\"State format: (step_count, agent_states, mobile_objects, mutable_objects)\")\n",
    "print(f\"\\nCurrent state:\")\n",
    "print(f\"  Step count: {state[0]}\")\n",
    "print(f\"  Agent states: {state[1]}\")\n",
    "print(f\"  Mobile objects: {state[2]}\")\n",
    "print(f\"  Mutable objects: {state[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a step and observe state change\n",
    "actions = [env.actions.forward] * len(env.agents)  # All agents move forward\n",
    "obs, rewards, done, info = env.step(actions)\n",
    "\n",
    "new_state = env.get_state()\n",
    "print(f\"After step:\")\n",
    "print(f\"  Step count: {new_state[0]}\")\n",
    "print(f\"  Done: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the original state\n",
    "env.set_state(state)\n",
    "restored_state = env.get_state()\n",
    "print(f\"State restored: {state == restored_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute the State DAG\n",
    "\n",
    "For finite environments, EMPO can compute the complete state-space DAG (Directed Acyclic Graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a very small environment for DAG computation (reduce max_steps for speed)\n",
    "from envs.one_or_three_chambers import SmallOneOrThreeChambersMapEnv\n",
    "\n",
    "small_env = SmallOneOrThreeChambersMapEnv()\n",
    "small_env.max_steps = 2  # Very small for quick demo\n",
    "small_env.reset()\n",
    "\n",
    "print(f\"Environment for DAG: {small_env.width}x{small_env.height} grid, {small_env.max_steps} max steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the DAG\n",
    "import time\n",
    "\n",
    "print(\"Computing state DAG...\")\n",
    "t0 = time.time()\n",
    "\n",
    "# get_dag() returns 3 values by default: states, state_to_idx, successors\n",
    "# Pass return_probabilities=True to also get transition probabilities as a 4th value\n",
    "states, state_to_idx, successors = small_env.get_dag()\n",
    "\n",
    "print(f\"✓ DAG computed in {time.time() - t0:.2f}s\")\n",
    "print(f\"  Total states: {len(states)}\")\n",
    "print(f\"  Terminal states: {sum(1 for s in successors if len(s) == 0)}\")\n",
    "print(f\"  Total transitions: {sum(len(s) for s in successors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Human Policy Prior\n",
    "\n",
    "The core EMPO computation: computing human policy priors via backward induction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Tuple\n",
    "\n",
    "# Define a simple goal: reaching a specific cell\n",
    "class ReachCellGoal(PossibleGoal):\n",
    "    \"\"\"A goal where a specific human agent tries to reach a specific cell.\"\"\"\n",
    "    \n",
    "    def __init__(self, world_model, human_agent_index: int, target_pos: tuple):\n",
    "        super().__init__(world_model)\n",
    "        self.human_agent_index = human_agent_index\n",
    "        self.target_pos = np.array(target_pos)\n",
    "    \n",
    "    def is_achieved(self, state) -> int:\n",
    "        \"\"\"Returns 1 if the agent is at the target position, 0 otherwise.\"\"\"\n",
    "        step_count, agent_states, mobile_objects, mutable_objects = state\n",
    "        \n",
    "        if self.human_agent_index < len(agent_states):\n",
    "            agent_state = agent_states[self.human_agent_index]\n",
    "            pos_x, pos_y = agent_state[0], agent_state[1]\n",
    "            if pos_x == self.target_pos[0] and pos_y == self.target_pos[1]:\n",
    "                return 1\n",
    "        return 0\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"ReachCell({self.human_agent_index}->({self.target_pos[0]},{self.target_pos[1]}))\"\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.human_agent_index, self.target_pos[0], self.target_pos[1]))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, ReachCellGoal):\n",
    "            return False\n",
    "        return (self.human_agent_index == other.human_agent_index and \n",
    "                np.array_equal(self.target_pos, other.target_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a goal generator that creates goals for reachable cells\n",
    "class SimpleCellGoalGenerator(PossibleGoalGenerator):\n",
    "    \"\"\"Generates goals for a few reachable cells.\"\"\"\n",
    "    \n",
    "    def __init__(self, world_model):\n",
    "        super().__init__(world_model)\n",
    "        \n",
    "        # Find empty cells (not walls)\n",
    "        self.empty_cells = []\n",
    "        for x in range(world_model.width):\n",
    "            for y in range(world_model.height):\n",
    "                cell = world_model.grid.get(x, y)\n",
    "                if cell is None or (hasattr(cell, 'type') and cell.type != 'wall'):\n",
    "                    self.empty_cells.append((x, y))\n",
    "        \n",
    "        # Limit to first 5 cells for demo speed\n",
    "        self.empty_cells = self.empty_cells[:5]\n",
    "        print(f\"Goal generator: {len(self.empty_cells)} target cells\")\n",
    "    \n",
    "    def generate(self, state, human_agent_index: int) -> Iterator[Tuple[PossibleGoal, float]]:\n",
    "        \"\"\"Yields goals with equal probability weights.\"\"\"\n",
    "        total_goals = len(self.empty_cells)\n",
    "        if total_goals == 0:\n",
    "            return\n",
    "        \n",
    "        weight = 1.0 / total_goals\n",
    "        for pos in self.empty_cells:\n",
    "            goal = ReachCellGoal(self.world_model, human_agent_index, pos)\n",
    "            yield goal, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify human agents (yellow color)\n",
    "human_agent_indices = []\n",
    "for i, agent in enumerate(small_env.agents):\n",
    "    if agent.color == 'yellow':\n",
    "        human_agent_indices.append(i)\n",
    "        \n",
    "print(f\"Found {len(human_agent_indices)} human agent(s): {human_agent_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create goal generator\n",
    "small_env.reset()\n",
    "goal_generator = SimpleCellGoalGenerator(small_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute human policy prior using backward induction\n",
    "print(\"Computing human policy prior...\")\n",
    "t0 = time.time()\n",
    "\n",
    "human_policy_prior = compute_human_policy_prior(\n",
    "    world_model=small_env,\n",
    "    human_agent_indices=human_agent_indices,\n",
    "    possible_goal_generator=goal_generator,\n",
    "    parallel=False,  # Single-threaded for Colab compatibility\n",
    "    level_fct=lambda state: state[0]  # Use step_count for level computation\n",
    ")\n",
    "\n",
    "print(f\"✓ Human policy prior computed in {time.time() - t0:.2f}s\")\n",
    "print(f\"  Type: {type(human_policy_prior).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the policy prior\n",
    "if len(human_agent_indices) > 0:\n",
    "    small_env.reset()\n",
    "    initial_state = small_env.get_state()\n",
    "    first_human_idx = human_agent_indices[0]\n",
    "    \n",
    "    # Get first goal\n",
    "    first_goal = None\n",
    "    for goal, weight in goal_generator.generate(initial_state, first_human_idx):\n",
    "        first_goal = goal\n",
    "        break\n",
    "    \n",
    "    if first_goal:\n",
    "        action_probs = human_policy_prior(initial_state, first_human_idx, first_goal)\n",
    "        print(f\"\\nAction probabilities for {first_goal}:\")\n",
    "        for action_idx, prob in enumerate(action_probs):\n",
    "            action_name = small_env.actions.available[action_idx] if action_idx < len(small_env.actions.available) else f\"action_{action_idx}\"\n",
    "            print(f\"  {action_name}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results\n",
    "\n",
    "Let's visualize a sample episode with random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample episode\n",
    "env.reset()\n",
    "frames = [env.render(mode='rgb_array', highlight=False)]\n",
    "\n",
    "done = False\n",
    "step = 0\n",
    "max_steps = 10\n",
    "\n",
    "while not done and step < max_steps:\n",
    "    actions = [env.action_space.sample() for _ in env.agents]\n",
    "    obs, rewards, done, info = env.step(actions)\n",
    "    frames.append(env.render(mode='rgb_array', highlight=False))\n",
    "    step += 1\n",
    "\n",
    "print(f\"Generated {len(frames)} frames over {step} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display frames as a grid\n",
    "n_frames = min(6, len(frames))\n",
    "fig, axes = plt.subplots(1, n_frames, figsize=(3*n_frames, 3))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    idx = i * (len(frames) - 1) // (n_frames - 1) if n_frames > 1 else 0\n",
    "    ax.imshow(frames[idx])\n",
    "    ax.set_title(f'Step {idx}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Sample Episode Frames', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optional Features\n",
    "\n",
    "### GPU Acceleration\n",
    "Colab provides free GPU access. The EMPO framework automatically uses GPU when available for neural network computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU status and create a simple tensor\n",
    "if torch.cuda.is_available():\n",
    "    x = torch.randn(1000, 1000, device='cuda')\n",
    "    print(f\"✓ GPU tensor created: {x.shape} on {x.device}\")\n",
    "else:\n",
    "    print(\"No GPU available - using CPU mode\")\n",
    "    print(\"To enable GPU: Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Policy Prior Training Demo\n",
    "This cell demonstrates training a neural policy prior using the `nn_based` package with GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Policy Prior Training using the EMPO nn_based package\n",
    "# This uses the same training infrastructure as examples/random_multigrid_ensemble_demo.py\n",
    "\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# Import EMPO neural network training components\n",
    "from empo.multigrid import MultiGridGoalSampler\n",
    "from empo.nn_based.multigrid import (\n",
    "    train_multigrid_neural_policy_prior,\n",
    "    MultiGridQNetwork,\n",
    ")\n",
    "\n",
    "# Device selection - use GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Training on: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Create a small environment for training demo\n",
    "# Using the same environment from earlier but with reduced max_steps for faster training\n",
    "from envs.one_or_three_chambers import SmallOneOrThreeChambersMapEnv\n",
    "\n",
    "train_env = SmallOneOrThreeChambersMapEnv()\n",
    "train_env.max_steps = 10  # Short episodes for demo\n",
    "train_env.reset()\n",
    "\n",
    "# Identify human agents (yellow color)\n",
    "train_human_indices = [i for i, agent in enumerate(train_env.agents) if agent.color == 'yellow']\n",
    "print(f\"Human agents: {train_human_indices}\")\n",
    "print(f\"Grid size: {train_env.width}x{train_env.height}\")\n",
    "\n",
    "# Create goal sampler - samples goals weighted by area\n",
    "goal_sampler = MultiGridGoalSampler(train_env)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Train the neural policy prior\n",
    "# This uses Q-learning with experience replay, similar to the demo scripts\n",
    "print(\"\\nTraining neural policy prior (50 episodes for demo)...\")\n",
    "t0 = time.time()\n",
    "\n",
    "neural_prior = train_multigrid_neural_policy_prior(\n",
    "    world_model=train_env,\n",
    "    human_agent_indices=train_human_indices,\n",
    "    goal_sampler=goal_sampler,\n",
    "    num_episodes=50,           # Short training for demo (use 500+ for real training)\n",
    "    steps_per_episode=10,      # Match max_steps\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-3,\n",
    "    gamma=0.99,                # Discount factor\n",
    "    beta=10.0,                 # Boltzmann temperature\n",
    "    replay_buffer_size=5000,\n",
    "    updates_per_episode=2,\n",
    "    epsilon=0.3,               # Exploration rate\n",
    "    reward_shaping=True,       # Use path-based reward shaping\n",
    "    device=device,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\n✓ Training completed in {elapsed:.2f}s\")\n",
    "print(f\"  Q-network type: {type(neural_prior.q_network).__name__}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in neural_prior.q_network.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained policy by getting action probabilities\n",
    "from empo.multigrid import ReachCellGoal\n",
    "\n",
    "train_env.reset()\n",
    "test_state = train_env.get_state()\n",
    "\n",
    "# Create a test goal (reach cell 3,3)\n",
    "if train_human_indices:\n",
    "    test_human_idx = train_human_indices[0]\n",
    "    test_goal = ReachCellGoal(train_env, test_human_idx, (3, 3))\n",
    "    \n",
    "    # Get action probabilities from the trained network (returns dict)\n",
    "    action_probs_dict = neural_prior(test_state, test_human_idx, test_goal)\n",
    "    \n",
    "    # Get action names from environment\n",
    "    if hasattr(train_env, 'actions') and hasattr(train_env.actions, 'available'):\n",
    "        action_names = train_env.actions.available\n",
    "    else:\n",
    "        action_names = [f'action_{i}' for i in range(len(action_probs_dict))]\n",
    "    \n",
    "    print(f\"Action probabilities for human {test_human_idx} to reach (3,3):\")\n",
    "    for action_idx in sorted(action_probs_dict.keys()):\n",
    "        prob = action_probs_dict[action_idx]\n",
    "        name = action_names[action_idx] if action_idx < len(action_names) else f'action_{action_idx}'\n",
    "        print(f\"  {name}: {prob:.4f}\")\n",
    "    \n",
    "    # Sample an action from the learned policy (convert dict to array)\n",
    "    probs_array = np.array([action_probs_dict[i] for i in sorted(action_probs_dict.keys())])\n",
    "    probs_array = probs_array / probs_array.sum()  # Normalize for numerical stability\n",
    "    sampled_action = int(np.random.choice(len(probs_array), p=probs_array))\n",
    "    \n",
    "    sampled_name = action_names[sampled_action] if sampled_action < len(action_names) else f'action_{sampled_action}'\n",
    "    print(f\"\\nSampled action: {sampled_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard Logging (Optional)\n",
    "Install tensorboard for experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install and use TensorBoard\n",
    "# !pip install -q tensorboard\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir outputs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights & Biases Integration (Optional)\n",
    "For more advanced experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install and use W&B\n",
    "# !pip install -q wandb\n",
    "# import wandb\n",
    "# wandb.login()  # Requires API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Caveats and Limitations in Colab\n",
    "\n",
    "### Not Supported:\n",
    "- **MPI distributed training**: Colab doesn't support MPI. Use single-process mode (`parallel=False`).\n",
    "- **Docker**: Colab runs in a containerized environment; you can't run Docker inside it.\n",
    "- **Long-running jobs**: Colab sessions timeout after ~12 hours (or sooner for free tier).\n",
    "\n",
    "### Workarounds:\n",
    "- For large state spaces, reduce `max_steps` or use smaller environments.\n",
    "- Save checkpoints frequently to Google Drive.\n",
    "- Use `parallel=False` for backward induction to avoid multiprocessing issues.\n",
    "\n",
    "### Tips:\n",
    "- Enable GPU for faster neural network training: Runtime > Change runtime type > GPU\n",
    "- Mount Google Drive to persist data across sessions\n",
    "- Use `%%time` magic to profile cell execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Mount Google Drive for data persistence\n",
    "# Uncomment to use:\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# # Save outputs to Drive\n",
    "# import shutil\n",
    "# shutil.copytree('outputs', '/content/drive/MyDrive/empo_outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Setup**: Cloning the repo, installing dependencies, configuring PYTHONPATH\n",
    "2. **Environment**: Creating and exploring MultiGrid environments\n",
    "3. **State Management**: Using `get_state()` and `set_state()` for exact state control\n",
    "4. **DAG Computation**: Computing the state-space structure\n",
    "5. **Policy Priors**: Computing human policy priors via backward induction\n",
    "6. **Visualization**: Rendering environment states\n",
    "7. **Neural Network Training**: GPU-accelerated training with TensorBoard logging\n",
    "\n",
    "For more information, see:\n",
    "- [GitHub Repository](https://github.com/mensch72/empo)\n",
    "- [API Documentation](https://github.com/mensch72/empo/blob/main/docs/API.md)\n",
    "- [README](https://github.com/mensch72/empo/blob/main/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EMPO Colab Demo Complete!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "empo_colab_demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
