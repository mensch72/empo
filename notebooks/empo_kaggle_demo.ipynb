{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a1036f",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Clone the repository and install dependencies. **Internet must be enabled.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e34c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the EMPO repository\n",
    "!git clone --depth 1 https://github.com/mensch72/empo.git\n",
    "%cd empo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486349a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the automated setup script\n",
    "# This installs dependencies and configures paths\n",
    "%run scripts/kaggle_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27509bf6",
   "metadata": {},
   "source": [
    "## 2. Verify GPU Setup\n",
    "\n",
    "Kaggle provides T4 or P100 GPUs. Let's verify GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2074d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"System Information:\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  NumPy version: {np.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"  ⚠️ No GPU available - training will be slower\")\n",
    "    print(\"  Enable GPU: Settings (right sidebar) → Accelerator → GPU\")\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"\\n✓ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ea14e",
   "metadata": {},
   "source": [
    "## 3. Create and Explore an Environment\n",
    "\n",
    "Let's create a simple multi-agent gridworld environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc46bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small environment for demonstration\n",
    "from envs.one_or_three_chambers import SmallOneOrThreeChambersMapEnv\n",
    "\n",
    "env = SmallOneOrThreeChambersMapEnv()\n",
    "env.reset()\n",
    "\n",
    "print(\"Environment Information:\")\n",
    "print(f\"  Grid size: {env.width} x {env.height}\")\n",
    "print(f\"  Number of agents: {len(env.agents)}\")\n",
    "print(f\"  Max steps: {env.max_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd9c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display agent information\n",
    "for i, agent in enumerate(env.agents):\n",
    "    agent_type = \"Human\" if agent.color == 'yellow' else \"Robot\" if agent.color == 'grey' else \"Other\"\n",
    "    can_push = getattr(agent, 'can_push_rocks', False)\n",
    "    print(f\"Agent {i} ({agent_type}): pos={tuple(agent.pos)}, color={agent.color}, can_push_rocks={can_push}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc99742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the environment\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = env.render(mode='rgb_array', highlight=False)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.title('Initial Environment State')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfec7a4",
   "metadata": {},
   "source": [
    "## 4. State Management\n",
    "\n",
    "EMPO extends MultiGrid with explicit state management for model-based planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current state (hashable, can be stored and restored)\n",
    "state = env.get_state()\n",
    "\n",
    "print(\"State format: (step_count, agent_states, mobile_objects, mutable_objects)\")\n",
    "print(f\"\\nCurrent state:\")\n",
    "print(f\"  Step count: {state[0]}\")\n",
    "print(f\"  Agent states: {state[1]}\")\n",
    "print(f\"  Mobile objects: {state[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take steps and observe state change\n",
    "actions = [env.actions.forward] * len(env.agents)\n",
    "obs, rewards, done, info = env.step(actions)\n",
    "\n",
    "new_state = env.get_state()\n",
    "print(f\"After step: step_count={new_state[0]}\")\n",
    "\n",
    "# Restore original state\n",
    "env.set_state(state)\n",
    "restored_state = env.get_state()\n",
    "print(f\"State restored: {state == restored_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e575fbac",
   "metadata": {},
   "source": [
    "## 5. Compute the State DAG\n",
    "\n",
    "For finite environments, EMPO computes the complete state-space DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87975842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create environment with reduced max_steps for speed\n",
    "small_env = SmallOneOrThreeChambersMapEnv()\n",
    "small_env.max_steps = 2  # Small for quick demo\n",
    "small_env.reset()\n",
    "\n",
    "print(f\"Environment: {small_env.width}x{small_env.height} grid, {small_env.max_steps} max steps\")\n",
    "\n",
    "# Compute the DAG\n",
    "print(\"Computing state DAG...\")\n",
    "t0 = time.time()\n",
    "states, state_to_idx, successors = small_env.get_dag()\n",
    "\n",
    "print(f\"✓ DAG computed in {time.time() - t0:.2f}s\")\n",
    "print(f\"  Total states: {len(states)}\")\n",
    "print(f\"  Terminal states: {sum(1 for s in successors if len(s) == 0)}\")\n",
    "print(f\"  Total transitions: {sum(len(s) for s in successors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ef242e",
   "metadata": {},
   "source": [
    "## 6. Compute Human Policy Prior\n",
    "\n",
    "The core EMPO computation: computing human policy priors via backward induction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9362b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from empo import PossibleGoal, PossibleGoalGenerator, compute_human_policy_prior\n",
    "from typing import Iterator, Tuple\n",
    "\n",
    "# Define a simple goal: reaching a specific cell\n",
    "class ReachCellGoal(PossibleGoal):\n",
    "    \"\"\"A goal where a specific human agent tries to reach a specific cell.\"\"\"\n",
    "    \n",
    "    def __init__(self, world_model, human_agent_index: int, target_pos: tuple):\n",
    "        super().__init__(world_model)\n",
    "        self.human_agent_index = human_agent_index\n",
    "        self.target_pos = np.array(target_pos)\n",
    "    \n",
    "    def is_achieved(self, state) -> int:\n",
    "        step_count, agent_states, mobile_objects, mutable_objects = state\n",
    "        if self.human_agent_index < len(agent_states):\n",
    "            agent_state = agent_states[self.human_agent_index]\n",
    "            if agent_state[0] == self.target_pos[0] and agent_state[1] == self.target_pos[1]:\n",
    "                return 1\n",
    "        return 0\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"ReachCell({self.human_agent_index}->({self.target_pos[0]},{self.target_pos[1]}))\"\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.human_agent_index, self.target_pos[0], self.target_pos[1]))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, ReachCellGoal):\n",
    "            return False\n",
    "        return (self.human_agent_index == other.human_agent_index and \n",
    "                np.array_equal(self.target_pos, other.target_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03294995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a goal generator\n",
    "class SimpleCellGoalGenerator(PossibleGoalGenerator):\n",
    "    \"\"\"Generates goals for reachable cells.\"\"\"\n",
    "    \n",
    "    def __init__(self, world_model):\n",
    "        super().__init__(world_model)\n",
    "        \n",
    "        # Find empty cells\n",
    "        self.empty_cells = []\n",
    "        for x in range(world_model.width):\n",
    "            for y in range(world_model.height):\n",
    "                cell = world_model.grid.get(x, y)\n",
    "                if cell is None or (hasattr(cell, 'type') and cell.type != 'wall'):\n",
    "                    self.empty_cells.append((x, y))\n",
    "        \n",
    "        self.empty_cells = self.empty_cells[:5]  # Limit for demo\n",
    "        print(f\"Goal generator: {len(self.empty_cells)} target cells\")\n",
    "    \n",
    "    def generate(self, state, human_agent_index: int) -> Iterator[Tuple[PossibleGoal, float]]:\n",
    "        total_goals = len(self.empty_cells)\n",
    "        if total_goals == 0:\n",
    "            return\n",
    "        weight = 1.0 / total_goals\n",
    "        for pos in self.empty_cells:\n",
    "            yield ReachCellGoal(self.world_model, human_agent_index, pos), weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c13a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify human agents and compute policy prior\n",
    "human_agent_indices = [i for i, agent in enumerate(small_env.agents) if agent.color == 'yellow']\n",
    "print(f\"Human agents: {human_agent_indices}\")\n",
    "\n",
    "small_env.reset()\n",
    "goal_generator = SimpleCellGoalGenerator(small_env)\n",
    "\n",
    "print(\"\\nComputing human policy prior...\")\n",
    "t0 = time.time()\n",
    "\n",
    "human_policy_prior = compute_human_policy_prior(\n",
    "    world_model=small_env,\n",
    "    human_agent_indices=human_agent_indices,\n",
    "    possible_goal_generator=goal_generator,\n",
    "    parallel=False,  # Single-threaded for Kaggle\n",
    "    level_fct=lambda state: state[0]\n",
    ")\n",
    "\n",
    "print(f\"✓ Human policy prior computed in {time.time() - t0:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0452e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the policy prior\n",
    "if human_agent_indices:\n",
    "    small_env.reset()\n",
    "    initial_state = small_env.get_state()\n",
    "    first_human_idx = human_agent_indices[0]\n",
    "    \n",
    "    # Get first goal\n",
    "    first_goal = next(iter(goal_generator.generate(initial_state, first_human_idx)))[0]\n",
    "    \n",
    "    action_probs = human_policy_prior(initial_state, first_human_idx, first_goal)\n",
    "    print(f\"Action probabilities for {first_goal}:\")\n",
    "    for action_idx, prob in enumerate(action_probs):\n",
    "        action_name = small_env.actions.available[action_idx] if action_idx < len(small_env.actions.available) else f\"action_{action_idx}\"\n",
    "        print(f\"  {action_name}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf215f99",
   "metadata": {},
   "source": [
    "## 7. Neural Network Training with GPU\n",
    "\n",
    "Train a neural policy prior using Kaggle's GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe536b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Import neural network training components\n",
    "from empo.multigrid import MultiGridGoalSampler\n",
    "from empo.nn_based.multigrid import train_multigrid_neural_policy_prior\n",
    "\n",
    "# Create training environment\n",
    "train_env = SmallOneOrThreeChambersMapEnv()\n",
    "train_env.max_steps = 10\n",
    "train_env.reset()\n",
    "\n",
    "train_human_indices = [i for i, agent in enumerate(train_env.agents) if agent.color == 'yellow']\n",
    "goal_sampler = MultiGridGoalSampler(train_env)\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# TensorBoard logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "log_dir = '/kaggle/working/tensorboard_logs'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "print(f\"Training environment: {train_env.width}x{train_env.height}, {len(train_human_indices)} human(s)\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"TensorBoard logs: {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural policy prior\n",
    "print(\"\\nTraining neural policy prior (100 episodes)...\")\n",
    "t0 = time.time()\n",
    "\n",
    "neural_prior = train_multigrid_neural_policy_prior(\n",
    "    world_model=train_env,\n",
    "    human_agent_indices=train_human_indices,\n",
    "    goal_sampler=goal_sampler,\n",
    "    num_episodes=100,          # Use 500+ for real training\n",
    "    steps_per_episode=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-3,\n",
    "    gamma=0.99,\n",
    "    beta=100.0,\n",
    "    replay_buffer_size=5000,\n",
    "    updates_per_episode=2,\n",
    "    epsilon=0.3,\n",
    "    reward_shaping=True,\n",
    "    device=device,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\n✓ Training completed in {elapsed:.2f}s\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in neural_prior.q_network.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894cb287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint to Kaggle output\n",
    "checkpoint_path = '/kaggle/working/neural_policy_prior.pt'\n",
    "torch.save({\n",
    "    'q_network_state_dict': neural_prior.q_network.state_dict(),\n",
    "    'training_episodes': 100,\n",
    "    'device': device,\n",
    "}, checkpoint_path)\n",
    "print(f\"✓ Model saved to: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c1aef",
   "metadata": {},
   "source": [
    "## 8. Visualize Episode\n",
    "\n",
    "Generate and visualize a sample episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e94915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate episode frames\n",
    "env.reset()\n",
    "frames = [env.render(mode='rgb_array', highlight=False)]\n",
    "\n",
    "done = False\n",
    "step = 0\n",
    "max_demo_steps = 10\n",
    "\n",
    "while not done and step < max_demo_steps:\n",
    "    actions = [env.action_space.sample() for _ in env.agents]\n",
    "    obs, rewards, done, info = env.step(actions)\n",
    "    frames.append(env.render(mode='rgb_array', highlight=False))\n",
    "    step += 1\n",
    "\n",
    "print(f\"Generated {len(frames)} frames over {step} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c997979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display frames as grid\n",
    "n_frames = min(6, len(frames))\n",
    "fig, axes = plt.subplots(1, n_frames, figsize=(3*n_frames, 3))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    idx = i * (len(frames) - 1) // (n_frames - 1) if n_frames > 1 else 0\n",
    "    ax.imshow(frames[idx])\n",
    "    ax.set_title(f'Step {idx}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Sample Episode', y=1.02)\n",
    "plt.savefig('/kaggle/working/episode_frames.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Saved to /kaggle/working/episode_frames.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186a1d4",
   "metadata": {},
   "source": [
    "## 9. Kaggle-Specific Tips\n",
    "\n",
    "### Resource Limits:\n",
    "- **GPU quota**: 30 hours/week (T4 or P100)\n",
    "- **Session length**: Max 12 hours\n",
    "- **Disk space**: 20GB in `/kaggle/working/`\n",
    "\n",
    "### Best Practices:\n",
    "- Save checkpoints to `/kaggle/working/` (persisted as output)\n",
    "- Use `torch.cuda.empty_cache()` to free GPU memory\n",
    "- Enable \"Save & Run All\" to produce downloadable outputs\n",
    "- Use datasets for large input files\n",
    "\n",
    "### Not Supported:\n",
    "- MPI distributed training (use `parallel=False`)\n",
    "- Docker containers\n",
    "- Long background processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory if needed\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d08e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files\n",
    "import os\n",
    "print(\"Output files in /kaggle/working/:\")\n",
    "for f in os.listdir('/kaggle/working/'):\n",
    "    size = os.path.getsize(f'/kaggle/working/{f}') / 1024\n",
    "    print(f\"  {f}: {size:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da6b41",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Setup**: Automated setup with `kaggle_setup.py`\n",
    "2. **Environment**: Creating MultiGrid environments\n",
    "3. **State Management**: `get_state()` / `set_state()` for planning\n",
    "4. **DAG Computation**: State-space exploration\n",
    "5. **Policy Priors**: Backward induction computation\n",
    "6. **Neural Training**: GPU-accelerated training with checkpoints\n",
    "7. **Visualization**: Episode rendering\n",
    "\n",
    "For more information:\n",
    "- [GitHub Repository](https://github.com/mensch72/empo)\n",
    "- [API Documentation](https://github.com/mensch72/empo/blob/main/docs/API.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f96dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EMPO Kaggle Demo Complete!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
