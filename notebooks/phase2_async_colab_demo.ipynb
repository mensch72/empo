{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2 Robot Policy Learning - Async Training Demo\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mensch72/empo/blob/main/notebooks/phase2_async_colab_demo.ipynb)\n",
        "\n",
        "This notebook demonstrates **Phase 2** of the EMPO framework - learning a robot policy\n",
        "that maximizes aggregate human power. Based on equations (4)-(9) from the paper.\n",
        "\n",
        "**Features:**\n",
        "- GPU-accelerated training\n",
        "- Async actor-learner architecture (optional)\n",
        "- Model-based targets for stable Q-learning\n",
        "- Warmup stages with buffer clearing\n",
        "\n",
        "**Networks trained:**\n",
        "- `Q_r`: Robot state-action value (eq. 4)\n",
        "- `V_h^e`: Human goal achievement under robot policy (eq. 6)\n",
        "- `X_h`: Aggregate goal achievement ability (eq. 7)\n",
        "- `U_r`: Intrinsic robot reward (eq. 8)\n",
        "- `V_r`: Robot state value (eq. 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "Clone the repository and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the EMPO repository\n",
        "!git clone --depth 1 https://github.com/mensch72/empo.git\n",
        "%cd empo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Python dependencies\n",
        "!pip install -q -r requirements-colab.txt\n",
        "print(\"Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up Python paths\n",
        "import sys\n",
        "import os\n",
        "\n",
        "repo_root = os.getcwd()\n",
        "sys.path.insert(0, os.path.join(repo_root, 'src'))\n",
        "sys.path.insert(0, os.path.join(repo_root, 'vendor', 'multigrid'))\n",
        "\n",
        "print(f\"PYTHONPATH configured\")\n",
        "print(f\"  Repository root: {repo_root}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Verify GPU and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"System Information\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"WARNING: No GPU available - training will be slower\")\n",
        "    print(\"   Go to Runtime -> Change runtime type -> GPU\")\n",
        "    device = 'cpu'\n",
        "\n",
        "print(f\"\\nUsing device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test EMPO imports\n",
        "print(\"Testing imports...\")\n",
        "\n",
        "from gym_multigrid.multigrid import MultiGridEnv, World, SmallActions\n",
        "print(\"  MultiGrid imports OK\")\n",
        "\n",
        "from empo.multigrid import MultiGridGoalSampler, ReachCellGoal\n",
        "from empo.possible_goal import TabularGoalSampler\n",
        "from empo.human_policy_prior import HeuristicPotentialPolicy\n",
        "from empo.nn_based.multigrid import PathDistanceCalculator\n",
        "print(\"  EMPO core imports OK\")\n",
        "\n",
        "from empo.nn_based.phase2.config import Phase2Config\n",
        "from empo.nn_based.multigrid.phase2 import train_multigrid_phase2\n",
        "print(\"  Phase 2 imports OK\")\n",
        "\n",
        "print(\"\\nAll imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Environment\n",
        "\n",
        "A simple grid with 1 robot (grey) that should help 1 human (yellow) reach their goal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment configuration\n",
        "GRID_MAP = \"\"\"\n",
        "We We We We We We\n",
        "We Ae Ro .. .. We\n",
        "We We Ay We We We\n",
        "We We We We We We\n",
        "\"\"\"\n",
        "# Ae = grey robot (agent), Ay = yellow human, Ro = rock, We = wall\n",
        "\n",
        "MAX_STEPS = 10\n",
        "FINAL_BETA_R = 100.0  # Robot policy concentration\n",
        "\n",
        "\n",
        "class Phase2DemoEnv(MultiGridEnv):\n",
        "    \"\"\"Simple grid environment for Phase 2 demo.\"\"\"\n",
        "    \n",
        "    def __init__(self, max_steps=MAX_STEPS):\n",
        "        super().__init__(\n",
        "            map=GRID_MAP,\n",
        "            max_steps=max_steps,\n",
        "            partial_obs=False,\n",
        "            objects_set=World,\n",
        "            actions_set=SmallActions\n",
        "        )\n",
        "        self.num_humans = sum(1 for a in self.agents if a.color == 'yellow')\n",
        "        self.num_robots = sum(1 for a in self.agents if a.color == 'grey')\n",
        "\n",
        "\n",
        "# Create and inspect environment\n",
        "env = Phase2DemoEnv()\n",
        "env.reset()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Environment\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Grid size: {env.width} x {env.height}\")\n",
        "print(f\"Max steps: {env.max_steps}\")\n",
        "print(f\"Action space: {env.action_space.n} actions\")\n",
        "\n",
        "# Identify agents\n",
        "human_indices = []\n",
        "robot_indices = []\n",
        "for i, agent in enumerate(env.agents):\n",
        "    if agent.color == 'yellow':\n",
        "        human_indices.append(i)\n",
        "        print(f\"Human {i}: pos={tuple(agent.pos)}\")\n",
        "    elif agent.color == 'grey':\n",
        "        robot_indices.append(i)\n",
        "        print(f\"Robot {i}: pos={tuple(agent.pos)}\")\n",
        "\n",
        "print(f\"\\n{len(human_indices)} human(s), {len(robot_indices)} robot(s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the environment\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = env.render(mode='rgb_array', highlight=False)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.imshow(img)\n",
        "plt.title('Initial Environment State')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Setup Human Policy and Goal Sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Setting up human policy and goal sampler...\")\n",
        "\n",
        "# Path calculator for heuristic policy\n",
        "path_calc = PathDistanceCalculator(\n",
        "    grid_height=env.height,\n",
        "    grid_width=env.width,\n",
        "    world_model=env\n",
        ")\n",
        "print(\"  Path calculator created\")\n",
        "\n",
        "# Human policy using HeuristicPotentialPolicy\n",
        "human_policy = HeuristicPotentialPolicy(\n",
        "    world_model=env,\n",
        "    human_agent_indices=human_indices,\n",
        "    path_calculator=path_calc,\n",
        "    beta=10.0  # Quite deterministic\n",
        ")\n",
        "print(\"  Human policy created\")\n",
        "\n",
        "# Goal sampler: human wants to reach cell (2,1) with 90% prob, (2,2) with 10%\n",
        "goal_sampler = TabularGoalSampler([\n",
        "    ReachCellGoal(env, 1, (2, 1)),\n",
        "    ReachCellGoal(env, 1, (2, 2))\n",
        "], probabilities=[0.9, 0.1])\n",
        "print(\"  Goal sampler created\")\n",
        "\n",
        "# Wrapper functions for trainer interface\n",
        "def goal_sampler_fn(state, human_idx):\n",
        "    goal, _ = goal_sampler.sample(state, human_idx)\n",
        "    return goal\n",
        "\n",
        "def human_policy_fn(state, human_idx, goal):\n",
        "    return human_policy.sample(state, human_idx, goal)\n",
        "\n",
        "print(\"\\nAll components ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configure Training\n",
        "\n",
        "Choose between **synchronous** and **async** training modes.\n",
        "\n",
        "**Note:** Async training with multiprocessing may not work reliably in Colab notebooks due to the `spawn` context limitations. If async fails, the notebook will fall back to synchronous training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training mode selection\n",
        "USE_ASYNC = False  # Set to True to try async training (may not work in Colab)\n",
        "QUICK_MODE = True  # Set to False for full training\n",
        "\n",
        "if QUICK_MODE:\n",
        "    NUM_EPISODES = 500\n",
        "    HIDDEN_DIM = 16\n",
        "    GOAL_FEATURE_DIM = 8\n",
        "    AGENT_EMBEDDING_DIM = 4\n",
        "    print(\"[QUICK MODE] Reduced episodes and network size\")\n",
        "else:\n",
        "    NUM_EPISODES = 3000\n",
        "    HIDDEN_DIM = 32\n",
        "    GOAL_FEATURE_DIM = 16\n",
        "    AGENT_EMBEDDING_DIM = 8\n",
        "    print(\"[FULL MODE] Full training configuration\")\n",
        "\n",
        "# Create configuration\n",
        "config = Phase2Config(\n",
        "    # Discount factors\n",
        "    gamma_r=0.95,\n",
        "    gamma_h=0.95,\n",
        "    \n",
        "    # Preference parameters\n",
        "    zeta=2.0,   # Risk aversion\n",
        "    xi=1.0,     # Inter-human inequality aversion\n",
        "    eta=1.1,    # Intertemporal inequality aversion\n",
        "    \n",
        "    # Policy concentration\n",
        "    beta_r=FINAL_BETA_R,\n",
        "    \n",
        "    # Exploration\n",
        "    epsilon_r_start=1.0,\n",
        "    epsilon_r_end=0.1,\n",
        "    epsilon_r_decay_steps=NUM_EPISODES * 10,\n",
        "    \n",
        "    # Learning rates\n",
        "    lr_q_r=1e-4,\n",
        "    lr_v_r=1e-4,\n",
        "    lr_v_h_e=1e-3,  # Higher for V_h_e (critical network)\n",
        "    lr_x_h=1e-4,\n",
        "    lr_u_r=1e-4,\n",
        "    \n",
        "    # Buffer and batching\n",
        "    buffer_size=10000,\n",
        "    batch_size=16,\n",
        "    x_h_batch_size=32,\n",
        "    \n",
        "    # Training loop\n",
        "    num_episodes=NUM_EPISODES,\n",
        "    steps_per_episode=MAX_STEPS,\n",
        "    updates_per_step=1,\n",
        "    goal_resample_prob=0.1,\n",
        "    \n",
        "    # Target networks\n",
        "    v_h_target_update_freq=100,\n",
        "    \n",
        "    # Model-based targets (for stable Q-learning)\n",
        "    use_model_based_targets=True,\n",
        "    \n",
        "    # Async training (optional)\n",
        "    async_training=USE_ASYNC,\n",
        "    num_actors=2,\n",
        "    actor_sync_freq=50,\n",
        "    async_min_buffer_size=200,\n",
        "    async_queue_size=5000,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training Configuration\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Episodes: {config.num_episodes}\")\n",
        "print(f\"Steps per episode: {config.steps_per_episode}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Async training: {config.async_training}\")\n",
        "if config.async_training:\n",
        "    print(f\"  Actors: {config.num_actors}\")\n",
        "    print(f\"  Sync frequency: {config.actor_sync_freq}\")\n",
        "print(f\"Model-based targets: {config.use_model_based_targets}\")\n",
        "print(f\"Network hidden dim: {HIDDEN_DIM}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train the Robot Policy\n",
        "\n",
        "This will train all Phase 2 networks with warmup stages:\n",
        "1. **Stage 1**: V_h_e only\n",
        "2. **Stage 2**: V_h_e + X_h\n",
        "3. **Stage 3**: V_h_e + X_h + U_r\n",
        "4. **Stage 4**: V_h_e + X_h + U_r + Q_r\n",
        "5. **beta_r ramping**: All networks with increasing policy concentration\n",
        "6. **Full training**: All networks with learning rate decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Phase 2 Robot Policy Training\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Output directory\n",
        "output_dir = 'outputs/phase2_colab_demo'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "tensorboard_dir = os.path.join(output_dir, 'tensorboard')\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "try:\n",
        "    robot_q_network, networks, history = train_multigrid_phase2(\n",
        "        world_model=env,\n",
        "        human_agent_indices=human_indices,\n",
        "        robot_agent_indices=robot_indices,\n",
        "        human_policy_prior=human_policy_fn,\n",
        "        goal_sampler=goal_sampler_fn,\n",
        "        config=config,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        goal_feature_dim=GOAL_FEATURE_DIM,\n",
        "        agent_embedding_dim=AGENT_EMBEDDING_DIM,\n",
        "        device=device,\n",
        "        verbose=True,\n",
        "        debug=False,\n",
        "        tensorboard_dir=tensorboard_dir\n",
        "    )\n",
        "    training_success = True\n",
        "    \n",
        "except Exception as e:\n",
        "    if config.async_training:\n",
        "        print(f\"\\nAsync training failed: {e}\")\n",
        "        print(\"Falling back to synchronous training...\\n\")\n",
        "        \n",
        "        # Retry with sync training\n",
        "        config.async_training = False\n",
        "        t0 = time.time()\n",
        "        \n",
        "        robot_q_network, networks, history = train_multigrid_phase2(\n",
        "            world_model=env,\n",
        "            human_agent_indices=human_indices,\n",
        "            robot_agent_indices=robot_indices,\n",
        "            human_policy_prior=human_policy_fn,\n",
        "            goal_sampler=goal_sampler_fn,\n",
        "            config=config,\n",
        "            hidden_dim=HIDDEN_DIM,\n",
        "            goal_feature_dim=GOAL_FEATURE_DIM,\n",
        "            agent_embedding_dim=AGENT_EMBEDDING_DIM,\n",
        "            device=device,\n",
        "            verbose=True,\n",
        "            debug=False,\n",
        "            tensorboard_dir=tensorboard_dir\n",
        "        )\n",
        "        training_success = True\n",
        "    else:\n",
        "        print(f\"\\nTraining failed: {e}\")\n",
        "        training_success = False\n",
        "        raise\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(f\"Training completed in {elapsed:.1f} seconds\")\n",
        "print(f\"  Mode: {'Async' if config.async_training else 'Synchronous'}\")\n",
        "print(f\"  Device: {device}\")\n",
        "print(f\"  Episodes: {len(history)}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show final loss values\n",
        "if history and len(history) > 0:\n",
        "    print(\"\\nLoss history (last 5 episodes):\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, losses in enumerate(history[-5:]):\n",
        "        episode_num = len(history) - 5 + i\n",
        "        loss_str = \", \".join(f\"{k}={v:.4f}\" for k, v in losses.items() if v > 0)\n",
        "        print(f\"Episode {episode_num}: {loss_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluate Learned Policy\n",
        "\n",
        "Run rollouts with the learned robot policy and visualize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Action names for display\n",
        "ACTION_NAMES = ['still', 'left', 'right', 'forward']\n",
        "\n",
        "def run_evaluation_rollout(env, robot_q_network, human_policy, goal_sampler, \n",
        "                           human_indices, robot_indices, device, config):\n",
        "    \"\"\"Run one rollout and return frames with Q-values.\"\"\"\n",
        "    robot_q_network.eval()\n",
        "    env.reset()\n",
        "    \n",
        "    # Sample goal\n",
        "    state = env.get_state()\n",
        "    human_goals = {}\n",
        "    for h in human_indices:\n",
        "        goal, _ = goal_sampler.sample(state, h)\n",
        "        human_goals[h] = goal\n",
        "    \n",
        "    frames = []\n",
        "    q_values_history = []\n",
        "    \n",
        "    for step in range(env.max_steps):\n",
        "        state = env.get_state()\n",
        "        \n",
        "        # Get Q-values\n",
        "        with torch.no_grad():\n",
        "            q_values = robot_q_network.encode_and_forward(state, env, device)\n",
        "            q_np = q_values.squeeze().cpu().numpy()\n",
        "            q_values_history.append(q_np.copy())\n",
        "        \n",
        "        # Get actions\n",
        "        actions = [0] * len(env.agents)\n",
        "        \n",
        "        for h in human_indices:\n",
        "            actions[h] = human_policy.sample(state, h, human_goals[h])\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            robot_action = robot_q_network.sample_action(\n",
        "                q_values, epsilon=0.0, beta_r=config.beta_r\n",
        "            )\n",
        "            for i, r in enumerate(robot_indices):\n",
        "                if i < len(robot_action):\n",
        "                    actions[r] = robot_action[i]\n",
        "        \n",
        "        # Render and step\n",
        "        frame = env.render(mode='rgb_array', highlight=False)\n",
        "        frames.append(frame)\n",
        "        \n",
        "        _, _, done, _ = env.step(actions)\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    return frames, q_values_history\n",
        "\n",
        "print(\"Running evaluation rollout...\")\n",
        "frames, q_history = run_evaluation_rollout(\n",
        "    env, robot_q_network, human_policy, goal_sampler,\n",
        "    human_indices, robot_indices, device, config\n",
        ")\n",
        "print(f\"Rollout completed: {len(frames)} steps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display Q-values over rollout\n",
        "print(\"\\nQ-values during rollout:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Step':>4}  \" + \"  \".join(f\"{name:>8}\" for name in ACTION_NAMES))\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for step, q_vals in enumerate(q_history):\n",
        "    best_action = np.argmax(q_vals)\n",
        "    q_str = \"  \".join(\n",
        "        f\"{q_vals[i]:>8.3f}\" + (\"*\" if i == best_action else \" \")\n",
        "        for i in range(len(ACTION_NAMES))\n",
        "    )\n",
        "    print(f\"{step:>4}  {q_str}\")\n",
        "\n",
        "print(\"\\n(* = selected action)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize rollout frames\n",
        "fig, axes = plt.subplots(1, min(len(frames), 6), figsize=(15, 3))\n",
        "if len(frames) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, ax in enumerate(axes):\n",
        "    if i < len(frames):\n",
        "        ax.imshow(frames[i])\n",
        "        ax.set_title(f'Step {i}')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Evaluation Rollout', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Generate Movie (Optional)\n",
        "\n",
        "Generate a video of multiple rollouts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_ROLLOUTS = 5\n",
        "MOVIE_FPS = 2\n",
        "\n",
        "print(f\"Generating {NUM_ROLLOUTS} rollouts...\")\n",
        "\n",
        "env.start_video_recording()\n",
        "\n",
        "for rollout_idx in range(NUM_ROLLOUTS):\n",
        "    env.reset()\n",
        "    \n",
        "    # Sample goal\n",
        "    state = env.get_state()\n",
        "    human_goals = {h: goal_sampler_fn(state, h) for h in human_indices}\n",
        "    \n",
        "    env.render(mode='rgb_array', highlight=False)\n",
        "    \n",
        "    for step in range(env.max_steps):\n",
        "        state = env.get_state()\n",
        "        actions = [0] * len(env.agents)\n",
        "        \n",
        "        for h in human_indices:\n",
        "            actions[h] = human_policy_fn(state, h, human_goals[h])\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            q_values = robot_q_network.encode_and_forward(state, env, device)\n",
        "            robot_action = robot_q_network.sample_action(q_values, epsilon=0.0, beta_r=config.beta_r)\n",
        "            for i, r in enumerate(robot_indices):\n",
        "                if i < len(robot_action):\n",
        "                    actions[r] = robot_action[i]\n",
        "        \n",
        "        _, _, done, _ = env.step(actions)\n",
        "        env.render(mode='rgb_array', highlight=False)\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    print(f\"  Rollout {rollout_idx + 1}/{NUM_ROLLOUTS} complete\")\n",
        "\n",
        "# Save movie\n",
        "movie_path = os.path.join(output_dir, 'phase2_demo.mp4')\n",
        "env.save_video(movie_path, fps=MOVIE_FPS)\n",
        "\n",
        "print(f\"\\nMovie saved to: {movie_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the video in Colab\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "if os.path.exists(movie_path):\n",
        "    with open(movie_path, 'rb') as f:\n",
        "        video_data = f.read()\n",
        "    video_b64 = b64encode(video_data).decode()\n",
        "    \n",
        "    html = f'''\n",
        "    <video width=\"400\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{video_b64}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    '''\n",
        "    display(HTML(html))\n",
        "else:\n",
        "    print(\"Movie file not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. TensorBoard (Optional)\n",
        "\n",
        "View training metrics in TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir outputs/phase2_colab_demo/tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **Environment Setup**: Simple gridworld with robot and human agents\n",
        "2. **Phase 2 Training**: Learning robot policy to maximize human power\n",
        "3. **Warmup Stages**: Progressive network activation for stable training\n",
        "4. **Model-Based Targets**: Using transition probabilities for consistent Q-values\n",
        "5. **Policy Evaluation**: Visualizing learned behavior\n",
        "\n",
        "**Key training features:**\n",
        "- Buffer clearing at warmup stage transitions\n",
        "- beta_r ramp-up for gradual policy concentration\n",
        "- Target networks for stable value estimation\n",
        "- Optional async training for scalability (works better as a script)\n",
        "\n",
        "For more details, see the [EMPO documentation](https://github.com/mensch72/empo)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
