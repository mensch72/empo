services:
  empo-dev:
    build:
      context: .
      network: host
      dockerfile: Dockerfile
      args:
        DEV_MODE: "true"
        USER_ID: ${USER_ID:-1000}
        GROUP_ID: ${GROUP_ID:-1000}
        HIERARCHICAL_MODE: ${HIERARCHICAL_MODE:-false}
    image: empo:dev
    pull_policy: build
    container_name: empo-dev
    
    volumes:
      - .:/workspace
      - pip-cache:/home/appuser/.cache/pip
    
    stdin_open: true
    tty: true
    
    ports:
      - "8888:8888"
      - "6006:6006"
      - "5678:5678"
    
    working_dir: /workspace
    
    environment:
      - WANDB_API_KEY=${WANDB_API_KEY:-}
      - PYTHONPATH=/workspace/src:/workspace/vendor/multigrid:/workspace/vendor/ai_transport
      - OLLAMA_HOST=${OLLAMA_HOST:-http://ollama:11434}
    
    command: /bin/bash

  # Optional Ollama service for local LLM inference
  # Start with: HIERARCHICAL_MODE=true docker compose --profile hierarchical up -d
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    profiles:
      - hierarchical
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    # Uncomment the following for GPU support:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

volumes:
  pip-cache:
  ollama-models:
