services:
  empo-dev:
    build:
      context: .
      network: host
      dockerfile: Dockerfile
      args:
        DEV_MODE: "true"
        USER_ID: ${USER_ID:-1001}
        GROUP_ID: ${GROUP_ID:-1001}
        HIERARCHICAL_MODE: ${HIERARCHICAL_MODE:-false}
    image: empo:dev-${USER_ID:-1001}
    pull_policy: build
    container_name: empo-dev
    # Run as the host user to avoid permission issues with mounted volumes
    user: "${USER_ID:-1001}:${GROUP_ID:-1001}"
    
    volumes:
      - .:/workspace
      - pip-cache:/home/appuser/.cache/pip
    
    stdin_open: true
    tty: true
    
    # Port mappings: host_port:container_port
    # Use HOST_TENSORBOARD_PORT=0 for auto-select, or set a specific port
    # Default 0 means Docker will choose an available port automatically
    ports:
      - "${HOST_JUPYTER_PORT:-8888}:8888"
      - "${HOST_TENSORBOARD_PORT:-0}:6006"
      - "${HOST_DEBUG_PORT:-5678}:5678"
    
    working_dir: /workspace
    
    environment:
      - WANDB_API_KEY=${WANDB_API_KEY:-}
      - PYTHONPATH=/workspace/src:/workspace/vendor/multigrid:/workspace/vendor/ai_transport
      - OLLAMA_HOST=${OLLAMA_HOST:-http://ollama:11434}
    
    command: /bin/bash

  # Optional Ollama service for local LLM inference
  # Start with: HIERARCHICAL_MODE=true docker compose --profile hierarchical up -d
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    profiles:
      - hierarchical
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    # Uncomment the following for GPU support:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

volumes:
  pip-cache:
  ollama-models:
